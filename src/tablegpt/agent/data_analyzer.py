from __future__ import annotations

from copy import deepcopy
from typing import TYPE_CHECKING
from uuid import uuid4

from langchain_core.messages import (
    BaseMessage,
    SystemMessage,
    ToolMessage,
    trim_messages,
)
from langchain_core.prompts import ChatPromptTemplate
from langgraph.graph import END, START, MessagesState, StateGraph
from langgraph.prebuilt import ToolNode

from tablegpt.agent.output_parser import MarkdownOutputParser
from tablegpt.tools import IPythonTool, markdown_console_template, process_content
from tablegpt.utils import filter_contents, format_columns

if TYPE_CHECKING:
    from datetime import date
    from pathlib import Path

    from langchain_core.agents import AgentAction, AgentFinish
    from langchain_core.retrievers import BaseRetriever
    from langchain_core.runnables import Runnable
    from pybox.base import BasePyBoxManager


INSTRUCTION = """You are 数海闻涛, an expert Python data analyst developed by 浙江大学计算机创新技术研究院 (Institute of Computer Innovation of Zhejiang University, or ZJUICI). Your job is to help user analyze datasets by writing Python code. Each markdown codeblock you write will be executed in an IPython environment, and you will receive the execution output. You should provide results analysis based on the execution output.
For politically sensitive questions, security and privacy issues, or other non-data analyze questions, you will refuse to answer.

Remember:
- Comprehend the user's requirements carefully & to the letter.
- If additional information is needed, feel free to ask the user.
- Give a brief description for what you plan to do & write Python code.
- You can use `read_df(uri: str) -> pd.DataFrame` function to read different file formats into DataFrame.
- When creating charts, prefer using `seaborn`.
- DO NOT include images using markdown syntax (![]()) in your response under ANY circumstances.
- If error occurred, try to fix it.
- Response in the same language as the user.
- Today is {date}"""

PROMPT = ChatPromptTemplate.from_messages(
    [
        ("system", INSTRUCTION),
        ("placeholder", "{messages}"),
    ]
)


def get_data_analyzer_agent(llm: Runnable) -> Runnable:
    return PROMPT | llm | MarkdownOutputParser(language_actions={"python": "python", "py": "python"})


class AgentState(MessagesState):
    # Current Date
    date: date

    # This is a bit of a hack to pass parent id to the agent state
    # But it act as the group id of all messages generated by the agent
    parent_id: str | None


def create_data_analyze_workflow(
    llm: Runnable,
    pybox_manager: BasePyBoxManager,
    *,
    workdir: Path | None = None,
    session_id: str | None = None,
    error_trace_cleanup: bool = False,
    vlm: Runnable | None = None,
    guard_chain: Runnable | None = None,
    dataset_retriever: BaseRetriever | None = None,
    verbose: bool = False,
) -> Runnable:
    """_summary_

    Args:
        llm (Runnable): _description_
        pybox_manager (BasePyBoxManager): _description_
        workdir (Path | None, optional): _description_. Defaults to None.
        session_id (str | None, optional): _description_. Defaults to None.
        error_trace_cleanup (bool, optional): _description_. Defaults to False.
        vlm (Runnable | None, optional): _description_. Defaults to None.
        guard_chain (Runnable | None, optional): _description_. Defaults to None.
        dataset_retriever (BaseRetriever | None, optional): _description_. Defaults to None.
        verbose (bool, optional): _description_. Defaults to False.

    Returns:
        Runnable: _description_
    """
    agent = get_data_analyzer_agent(llm)

    vlm_agent = None
    if vlm is not None:
        vlm_agent = get_data_analyzer_agent(vlm)

    async def run_input_guard(state: AgentState) -> dict[str, list[BaseMessage]]:
        if guard_chain is not None:
            last_message = state["messages"][-1]
            flag, category = await guard_chain.ainvoke(input={"input": last_message.content})
            if flag == "unsafe" and category is not None:
                # TODO: "敏感话题"?
                content = f"用户问题可能涉及与 `{category}` 相关的敏感话题，请谨慎回答。"  # noqa: RUF001
                return {
                    "messages": [
                        SystemMessage(
                            id=str(uuid4()),
                            content=content,
                            additional_kwargs={"parent_id": state["parent_id"]},
                        )
                    ]
                }
        return {"messages": []}

    async def retrieve_columns(state: AgentState) -> dict:
        if dataset_retriever is None:
            return {"messages": []}

        last_message = state["messages"][-1]
        docs = await dataset_retriever.ainvoke(
            input=last_message.content,
        )
        formatted = format_columns(docs)
        return {
            "messages": [
                SystemMessage(
                    id=str(uuid4()),
                    content=formatted,
                    additional_kwargs={"parent_id": state["parent_id"]},
                )
            ]
        }

    tools = [
        IPythonTool(
            pybox_manager=pybox_manager,
            cwd=workdir,
            session_id=session_id,
            error_trace_cleanup=error_trace_cleanup,
        )
    ]
    tool_executor = ToolNode(tools)

    async def arun_tablegpt_agent(state: AgentState) -> dict:
        # Truncate messages based on message count.
        # TODO: truncate based on token count.
        windowed_messages = trim_messages(
            state["messages"],
            token_counter=len,
            max_tokens=20,
            start_on="human",  # This means that the first message should be from the user after trimming.
            # The system message is not in `messages`, so we don't need to specify `include_system`
        )
        # Keep only 'text' and 'table' content
        filtered_messages = filter_contents(windowed_messages, keep={"text", "table"})

        # Extract filename from attachments to content
        temp_messages = deepcopy(filtered_messages)
        for message in temp_messages:
            if attachments := message.additional_kwargs.get("attachments"):
                # TODO: We only support one attachment for now.
                message.content = f"文件名称: {attachments[0]['filename']}"

        agent_outcome: AgentAction | AgentFinish = await agent.ainvoke(
            {
                "messages": temp_messages,
                "date": state["date"],
            }
        )

        messages = []
        for message in agent_outcome.messages:
            message.additional_kwargs["parent_id"] = state["parent_id"]
            messages.append(message)
        return {"messages": messages}

    async def arun_vlm_agent(state: AgentState) -> dict:
        # Truncate messages based on message count.
        # TODO: truncate based on token count.
        windowed_messages: list[BaseMessage] = trim_messages(
            state["messages"],
            token_counter=len,
            max_tokens=20,
            start_on="human",  # This means that the first message should be from the user after trimming.
            # The system message is not in `messages`, so we don't need to specify `include_system`
        )
        # NOTE: This is hacky, but VLMs have limits on the number of images they can process.
        # First we keep only 'text' part for all windowed messages except the last one.
        filtered_messages = filter_contents(windowed_messages[:-1], keep={"text"})
        # Then we add the image content of the last message back, keep it under `max_support_images`.
        if isinstance(windowed_messages[-1].content, str):
            last_message = windowed_messages[-1]
        else:
            max_support_images = int((vlm.metadata or {}).get("max_support_images", 5))
            last_message: BaseMessage = deepcopy(windowed_messages[-1])
            last_message.content = []
            added = 0
            for part in reversed(windowed_messages[-1].content):
                if isinstance(part, str):
                    last_message.content.insert(0, part)
                    continue
                if part.get("type") == "text":
                    last_message.content.insert(0, part)
                    continue
                if part.get("type") == "image_url" and added < max_support_images:
                    last_message.content.insert(0, part)
                    added += 1
        filtered_messages.append(last_message)

        # Extract filename from attachments to content
        temp_messages = deepcopy(filtered_messages)
        for message in temp_messages:
            if attachments := message.additional_kwargs.get("attachments"):
                # TODO: We only support one attachment for now.
                message.content = f"文件名称: {attachments[0]['filename']}"

        agent_outcome: AgentAction | AgentFinish = await vlm_agent.ainvoke(
            {
                "messages": temp_messages,
                "date": state["date"],
            }
        )
        messages = []
        for message in agent_outcome.messages:
            message.additional_kwargs["parent_id"] = state["parent_id"]
            messages.append(message)
        return {"messages": messages}

    async def tool_node(state: AgentState) -> dict:
        messages: list[ToolMessage] = await tool_executor.ainvoke(state["messages"])
        for message in messages:
            message.additional_kwargs = message.additional_kwargs | {
                "parent_id": state["parent_id"],
            }
            # TODO: we assume our tool is only IPythonTool, so we can hardcode the format here.
            message.content = process_content(message.content)
            for part in message.content:
                if isinstance(part, dict) and part.get("type") == "text":
                    part["text"] = markdown_console_template.format(res=part["text"])
        return {"messages": messages}

    def should_continue(state: AgentState) -> str:
        # Must have at least one message when entering this router
        last_message = state["messages"][-1]
        if last_message.tool_calls:
            return "tools"
        return "end"

    def agent_selector(state: AgentState) -> str:
        if vlm_agent is None:
            return "agent"

        # No messages yet. We should start with the default agent
        if len(state["messages"]) < 1:
            return "agent"

        # If the latest message contains "image/xxx" output,
        # the workflow graph shoud route to "vlm_agent"
        last_message = state["messages"][-1]
        for part in last_message.content:
            if isinstance(part, dict) and part.get("type") == "image_url":
                return "vlm_agent"
        return "agent"

    workflow = StateGraph(AgentState)

    workflow.add_node("input_guard", run_input_guard)
    workflow.add_node("retrieve_columns", retrieve_columns)
    workflow.add_node("agent", arun_tablegpt_agent)
    workflow.add_node("vlm_agent", arun_vlm_agent)
    workflow.add_node("tools", tool_node)

    workflow.add_edge(START, "input_guard")
    workflow.add_edge(START, "retrieve_columns")
    workflow.add_edge(["input_guard", "retrieve_columns"], "agent")

    workflow.add_conditional_edges("tools", agent_selector)
    workflow.add_conditional_edges(
        "agent",
        should_continue,
        {
            "tools": "tools",
            "end": END,
        },
    )
    workflow.add_conditional_edges(
        "vlm_agent",
        should_continue,
        {
            "tools": "tools",
            "end": END,
        },
    )
    return workflow.compile(debug=verbose)
